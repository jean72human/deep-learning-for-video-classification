{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NKJhfg4ZpkT"
   },
   "source": [
    "#LibiumNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "tkH0NM_xmmE7",
    "outputId": "fff4b4fb-e658-40e9-e817-e14ae9a6ebbb"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, glob\n",
    "import imageio\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, TimeDistributed, LSTM, Input, BatchNormalization, Conv2D, MaxPooling2D, Reshape, Conv1D, GlobalAveragePooling1D, MaxPooling1D, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "_z6oXdzSrj4A",
    "outputId": "4de8c327-985d-48b2-e586-ebcbc7e35af1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "K13X7Hdg3Ktp",
    "outputId": "c5403c56-c284-4e6d-bb55-89785137ae05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reading import read_gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-fZdjfygGXe"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This model generates generator of the datasets for the Network. \n",
    "\n",
    "@authors : Mustapha Tidoo Yussif, Samuel Atule, Jean Sabastien Dovonon\n",
    "         and Nutifafa Amedior. \n",
    "\"\"\"\n",
    "IMAGE_HEIGHT = 200\n",
    "IMAGE_WIDTH = 200\n",
    "IMAGE_CHANNEL = 3\n",
    "NUM_FRAMES = 50\n",
    "NUM_CLASSES = 2\n",
    "        \n",
    "        \n",
    "class GenerateDataset(object):\n",
    "    \"\"\"Generates generator for the datasets\n",
    "    \n",
    "    This model generates a generator for the datasets. This done to efficiently \n",
    "    manage space.\n",
    "    \n",
    "    :param: file_path: path to files/videos.\n",
    "    :param directory: Path to the main directory.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, directory, n_items):\n",
    "        self.n_items = n_items\n",
    "        self.directory = directory\n",
    "        self.file_path = file_path\n",
    "        self.num_samples = len(self.samples(self.get_video_files(self.file_path, self.directory)))\n",
    "        \n",
    "\n",
    "    def load_video(self, filename):\n",
    "        \"\"\"Loads the specified video.\n",
    "\n",
    "        Returns:\n",
    "            List[FloatTensor]: the frames of the video as a list of 3D tensors\n",
    "                (channels, width, height)\"\"\"\n",
    "        \n",
    "        return read_gif(filename)\n",
    "    \n",
    "\n",
    "    def get_sample_size(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    \n",
    "    def create_df(self, file_path):\n",
    "        '''\n",
    "        creates pandas dataframe of labels and actions directories\n",
    "        '''\n",
    "        \n",
    "        d = {}\n",
    "        y_labels = []\n",
    "        class_folders = []\n",
    "        for ind, clss in enumerate(os.listdir(file_path)):\n",
    "            y_labels.append(ind)\n",
    "            class_folders.append(clss)\n",
    "        \n",
    "        d['directory'] = class_folders\n",
    "        d['class'] = y_labels\n",
    "        print(d)\n",
    "        return pd.DataFrame(d)\n",
    "\n",
    "\n",
    "    def get_video_files(self, file_path, directory=None):\n",
    "        '''\n",
    "        get video files from word class directories\n",
    "        '''\n",
    "        d = {}\n",
    "        f = []\n",
    "        \n",
    "        for root, dirs, files in os.walk(file_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".gif\"):\n",
    "                    target_file = file.split('_')[0]\n",
    "                    f.append(target_file)\n",
    "                    if target_file not in d:\n",
    "                        d[target_file] = []\n",
    "                    d[target_file].append(os.path.join(root, file))\n",
    "        return d\n",
    "        \n",
    "    def generator(self):\n",
    "        \"\"\"Interfaces the private generator method\n",
    "\n",
    "        :param num_items_per_class: The number of items in a categority. \n",
    "        :param batch: The batch size.\n",
    "        \"\"\"\n",
    "        data = self.create_df(self.file_path)\n",
    "        video_files = self.get_video_files(self.file_path, self.directory)\n",
    "        return self._generator(data, directory = self.directory, video_files = video_files)\n",
    "\n",
    "    def samples(self, video_files):\n",
    "        train = []\n",
    "        for key, value in video_files.items():\n",
    "            ind = 0\n",
    "            for file in value:\n",
    "                train.append(file)\n",
    "                ind+=1\n",
    "                if ind == self.n_items:\n",
    "                    break\n",
    "\n",
    "        return train\n",
    "    \n",
    "    def _generator(self, data, directory=None, video_files=None, BATCH_SIZE = 1):\n",
    "        \n",
    "        '''\n",
    "        retrieves the training batch for each iteration\n",
    "        '''\n",
    "        \n",
    "        train = []\n",
    "        for key, value in video_files.items():\n",
    "            ind = 0\n",
    "            for file in value:\n",
    "                train.append(file)\n",
    "                ind+=1\n",
    "                if ind == self.n_items:\n",
    "                    break\n",
    "                \n",
    "                  \n",
    "                \n",
    "        while True:\n",
    "            # Randomize the indices to make an array\n",
    "            indices_arr = np.random.permutation(len(train))\n",
    "            \n",
    "            for batch in range(0, len(indices_arr), BATCH_SIZE):\n",
    "                # slice out the current batch according to batch-size\n",
    "                current_batch = indices_arr[batch:(batch + BATCH_SIZE)]\n",
    "\n",
    "                # initializing the arrays, x_train and y_train\n",
    "                x_train = np.empty([0, NUM_FRAMES, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL], dtype=np.float32)\n",
    "            \n",
    "                y_train = np.empty([0], dtype=np.int32)\n",
    "\n",
    "                for i in current_batch:\n",
    "                    # get an image and its corresponding color for an traffic light\n",
    "                    video_frames = self.load_video(train[i])\n",
    "                    \n",
    "                    \n",
    "                    #preprocess frames from videos\n",
    "#                     video_frames = tf.image.resize_nearest_neighbor(video_frames,(IMAGE_HEIGHT, IMAGE_WIDTH), )\n",
    "                    #video_frames = tf.image.rgb_to_grayscale(video_frames)\n",
    "#                     video_frames = tf.reshape(video_frames, (NUM_FRAMES, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL))\n",
    "\n",
    "                    # Appending them to existing batch\n",
    "                    x_train = np.append(x_train, [video_frames/255], axis=0)\n",
    "                    tvar = train[i].split(\"\\\\\")[0].split(\"/\")[-1]\n",
    "                    y_train = np.append(y_train, [ data.loc[ data['directory'] == tvar ].values[0][1] ])\n",
    "                    #print(data.loc[ data['directory'] == train[i].split('/')[-1].split('_')[-2] ].values[0][1])\n",
    "                    \n",
    "                \n",
    "                y_train = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "                \n",
    "                yield(x_train, y_train)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeV-IN7ZY6U6"
   },
   "outputs": [],
   "source": [
    "class LibiumNet(object):\n",
    "    \"\"\"TA lipreading model, `LibiunNet`\n",
    "    This is lip reading model which reads or predicts the words of a spoken mouth in a silent video. \n",
    "    This model implements the RCNN (Recurrent Convolutional Neural Network) architecture. \n",
    "\n",
    "    :param img_c: The number of channels of the input image. i.e. a frame in a video (default 3).\n",
    "    :param img_w: The width of the input image i.e. a frame in a video (default 256)\n",
    "    :param img_h: The height of the input image i.e. a frame in a video (default 256)\n",
    "    :param frames_n: The total number of frames in an input video (default 29)\n",
    "    :param output_size: The output size of the network. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, img_c=3, img_w=IMAGE_WIDTH, img_h=IMAGE_HEIGHT, frames_n=NUM_FRAMES, output_size=NUM_CLASSES):\n",
    "        self.img_c = img_c\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.frames_n = frames_n\n",
    "        self.output_size = output_size\n",
    "        self.history = None\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Retrieves the features from the last pool layer in the densenet pretrained model \n",
    "        and pass obtained features to LSTM network. \n",
    "        \"\"\"\n",
    "        input_shape = (self.frames_n, self.img_w, self.img_h, self.img_c) # input shape\n",
    "   \n",
    "        \n",
    "        feature_extractor = Sequential()\n",
    "        inputShape = (self.img_w, self.img_h, self.img_c)\n",
    "        chanDim = -1\n",
    "        \n",
    "        feature_extractor.add(Lambda(lambda x: tf.keras.backend.mean(x, axis=3, keepdims=True), input_shape=inputShape))\n",
    "        feature_extractor.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # first CONV => RELU => CONV => RELU => POOL layer set\n",
    "        \n",
    "        feature_extractor.add(Conv2D(32, (3, 3)))\n",
    "        feature_extractor.add(Activation(\"relu\"))\n",
    "        feature_extractor.add(BatchNormalization(axis=chanDim))\n",
    "        feature_extractor.add(Conv2D(32, (3, 3)))\n",
    "        feature_extractor.add(Activation(\"relu\"))\n",
    "        feature_extractor.add(BatchNormalization(axis=chanDim))\n",
    "        feature_extractor.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        feature_extractor.add(Dropout(0.25))\n",
    "        \n",
    "        # second CONV => RELU => CONV => RELU => POOL layer set\n",
    "        feature_extractor.add(Conv2D(64, (3, 3)))\n",
    "        feature_extractor.add(Activation(\"relu\"))\n",
    "        feature_extractor.add(BatchNormalization(axis=chanDim))\n",
    "        feature_extractor.add(Conv2D(64, (3, 3)))\n",
    "        feature_extractor.add(Activation(\"relu\"))\n",
    "        feature_extractor.add(BatchNormalization(axis=chanDim))\n",
    "        feature_extractor.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        feature_extractor.add(Dropout(0.25))\n",
    "        \n",
    "        \n",
    "        TIME_PERIODS = self.frames_n\n",
    "        dims = 53824\n",
    "\n",
    "        model_m = Sequential()\n",
    "        model_m.add(Conv1D(10, 2, activation='relu'))\n",
    "        model_m.add(Conv1D(10, 2, activation='relu'))\n",
    "        \n",
    "        \n",
    "        self.input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n",
    "        self.image_frame_features = TimeDistributed(feature_extractor)(self.input_data) ## extracting the features from the images\n",
    "        \n",
    "        self.flat = TimeDistributed(Flatten())(self.image_frame_features) ## flatten before passing on to the recurrent network\n",
    "\n",
    "        self.sequence = Flatten()(model_m(self.flat))\n",
    "        \n",
    "        self.dense = Dense(self.output_size, activation='softmax')(self.sequence)\n",
    "\n",
    "\n",
    "        self.model = Model(inputs = self.input_data, outputs=self.dense)\n",
    "\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\"Summarizes the architecture of the model.\n",
    "        \n",
    "        :return: returns the model architecture summary\n",
    "        \"\"\"\n",
    "        return self.model.summary()\n",
    "      \n",
    "    \n",
    "    def train(self, generator,steps_per_epoch=None, epochs=1,validation_data=None, validation_steps=None):\n",
    "        # Callbacks\n",
    "        early_stopping_monitor = EarlyStopping(patience=3)\n",
    "        callbacks_list = [early_stopping_monitor]\n",
    "\n",
    "        \n",
    "        print('Training...')\n",
    "        \n",
    "        self.model.compile(\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.history = self.model.fit(generator, steps_per_epoch=steps_per_epoch, epochs=epochs, validation_data=validation_data, validation_steps = validation_steps)\n",
    "        \n",
    "        #self.visualize_accuracy(history)\n",
    "        #self.visualize_loss(history)\n",
    "      \n",
    "      \n",
    "    def predict(self, input_batch):\n",
    "        \"\"\"Predicts a video\n",
    "        \n",
    "        :param input_batch: A batch of a sequence of frames. \n",
    "        :return: returns the predicted probailities\n",
    "        \"\"\"\n",
    "        return self.model(input_batch)\n",
    "      \n",
    "    def visualize_accuracy(self):\n",
    "        \"\"\"Visualize model accuracy\n",
    "        \"\"\"\n",
    "        if self.history:\n",
    "            plt.plot(self.history.history['accuracy'], label='training accuracy')\n",
    "            plt.plot(self.history.history['val_accuracy'], label='testing accuracy')\n",
    "            plt.title('Accuracy')\n",
    "            plt.xlabel('epochs')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.legend()\n",
    "      \n",
    "    def visualize_loss(self):\n",
    "        \"\"\"Visualizes model loss\"\"\"\n",
    "        if self.history:\n",
    "            plt.plot(self.history.history['loss'], label='training loss')\n",
    "            plt.plot(self.history.history['val_loss'], label='testing loss')\n",
    "            plt.title('Loss')\n",
    "            plt.xlabel('epochs')\n",
    "            plt.ylabel('loss')\n",
    "            plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "id": "KorXcvrLh229",
    "outputId": "e164814e-a81c-4ae4-d786-008806ceceab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       [(None, 50, 200, 200, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 50, 22, 22, 64)    65760     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 30976)         0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 48, 10)            619740    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 962       \n",
      "=================================================================\n",
      "Total params: 686,462\n",
      "Trainable params: 686,078\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = LibiumNet(output_size=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "gE_yz0glgYMS",
    "outputId": "679b94eb-9e2c-439e-a37f-afb41c1d879e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'directory': ['kick', 'punch'], 'class': [0, 1]}\n",
      "{'directory': ['kick', 'punch'], 'class': [0, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = GenerateDataset('data/train/', '', 40)\n",
    "#gen = GenerateDataset('/gdrive/My Drive/LibiumNet/overfit_test/', 'train')\n",
    "datasets = gen.generator()\n",
    "num_samples = gen.get_sample_size()\n",
    "steps_per_epoch = 5\n",
    "\n",
    "# validation\n",
    "val_gen = GenerateDataset('data/val/', '', 10)\n",
    "#val_gen = GenerateDataset('/gdrive/My Drive/LibiumNet/overfit_test/', 'train')\n",
    "val_datasets = val_gen.generator()\n",
    "num_valid_samples = val_gen.get_sample_size()\n",
    "steps_per_valid_epoch = 5\n",
    "\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1716
    },
    "colab_type": "code",
    "id": "u-jhq3Q6d6TZ",
    "outputId": "7b6713c1-4a8e-48f6-f56f-d51488ce2f58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 2s 451ms/step - loss: 0.8628 - accuracy: 0.4000 - val_loss: 0.6925 - val_accuracy: 0.6000\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 0.7153 - accuracy: 0.4000 - val_loss: 0.6973 - val_accuracy: 0.4000\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 421ms/step - loss: 1.0168 - accuracy: 0.4000 - val_loss: 0.6916 - val_accuracy: 0.6000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 453ms/step - loss: 0.1702 - accuracy: 1.0000 - val_loss: 0.6978 - val_accuracy: 0.4000\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.2504 - accuracy: 1.0000 - val_loss: 0.6985 - val_accuracy: 0.4000\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 358ms/step - loss: 0.1782 - accuracy: 1.0000 - val_loss: 0.6850 - val_accuracy: 0.6000\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 425ms/step - loss: 0.2898 - accuracy: 0.8000 - val_loss: 0.7274 - val_accuracy: 0.2000\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 410ms/step - loss: 0.2035 - accuracy: 0.8000 - val_loss: 0.6536 - val_accuracy: 0.8000\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.7069 - val_accuracy: 0.4000\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 403ms/step - loss: 0.0651 - accuracy: 1.0000 - val_loss: 0.7015 - val_accuracy: 0.4000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 401ms/step - loss: 0.0751 - accuracy: 1.0000 - val_loss: 0.6753 - val_accuracy: 0.6000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 456ms/step - loss: 0.0796 - accuracy: 1.0000 - val_loss: 0.7360 - val_accuracy: 0.2000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.0861 - accuracy: 1.0000 - val_loss: 0.7047 - val_accuracy: 0.4000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 321ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.7123 - val_accuracy: 0.4000\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 489ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.6369 - val_accuracy: 0.8000\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.6371 - val_accuracy: 0.8000\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.7543 - val_accuracy: 0.2000\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 485ms/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 0.6697 - val_accuracy: 0.6000\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.7651 - val_accuracy: 0.2000\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 15s 3s/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.7166 - val_accuracy: 0.4000\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 53s 11s/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.6710 - val_accuracy: 0.6000\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 53s 11s/step - loss: 0.1317 - accuracy: 1.0000 - val_loss: 0.7665 - val_accuracy: 0.2000\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 47s 9s/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.6004 - val_accuracy: 0.8000\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 3s 609ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.7329 - val_accuracy: 0.4000\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 3s 546ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.7271 - val_accuracy: 0.4000\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.6627 - val_accuracy: 0.6000\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.7307 - val_accuracy: 0.4000\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 3s 600ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.7980 - val_accuracy: 0.2000\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 42s 8s/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5958 - val_accuracy: 0.8000\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 554s 111s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6704 - val_accuracy: 0.6000\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 0.7237 - val_accuracy: 0.4000\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 5s 1s/step - loss: 2.8297e-04 - accuracy: 1.0000 - val_loss: 0.6698 - val_accuracy: 0.6000\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 7s 1s/step - loss: 6.1784e-04 - accuracy: 1.0000 - val_loss: 0.7200 - val_accuracy: 0.4000\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 4s 846ms/step - loss: 6.8439e-04 - accuracy: 1.0000 - val_loss: 0.5882 - val_accuracy: 0.8000\n",
      "Epoch 35/100\n",
      "2/5 [===========>..................] - ETA: 1s - loss: 0.0017 - accuracy: 1.0000    "
     ]
    }
   ],
   "source": [
    " # training \n",
    "epochs = 40\n",
    "model.train(datasets, steps_per_epoch = steps_per_epoch, epochs=epochs,validation_data=val_datasets, validation_steps=steps_per_valid_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "g7pFUnVHKtiF",
    "outputId": "66a26818-64e7-4f7a-dff3-5887757bc188"
   },
   "outputs": [],
   "source": [
    "#model.model.save('libium.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LibiumNet.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
